version: '3.8'

services:
  # The Kafka topic initializer service.
  # It waits for Kafka to be healthy and then creates the necessary topics.
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      kafka1:
        condition: service_healthy
    entrypoint: >
      bash -c "
        # Wait until Kafka is ready, using the bootstrap-server defined in our Kafka service
        cub kafka-ready -b kafka1:9092 1 20 &&
        
        # Create user.login topic
        kafka-topics --create --if-not-exists \
        --topic user.login \
        --bootstrap-server kafka1:9092 \
        --partitions 1 --replication-factor 1 &&

        # Create rec.request topic
        kafka-topics --create --if-not-exists \
        --topic rec.request \
        --bootstrap-server kafka1:9092 \
        --partitions 1 --replication-factor 1 &&
        
        # Create llm.raw topic
        kafka-topics --create --if-not-exists \
        --topic llm.raw \
        --bootstrap-server kafka1:9092 \
        --partitions 1 --replication-factor 1 &&
        
        # Create rec.ready topic
        kafka-topics --create --if-not-exists \
        --topic rec.ready \
        --bootstrap-server kafka1:9092 \
        --partitions 1 --replication-factor 1
      "
    networks:
      - app-network

  # The Python/FastAPI authentication service
  auth:
    build:
      context: ./services/auth
      dockerfile: Dockerfile
    container_name: media-recs-auth
    ports:
      - "8000:8000"
    volumes:
      - ./services/auth:/app
    env_file:
      - ./services/auth/.env
    networks:
      - app-network

  # Build the shared base image that other services will use
  base-image:
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile.base
    image: genaimediarecommendation-base:latest

  # The LLM service
  llm-service:
    # Use the pre-built base image, but specify the build context again
    # to find the correct Dockerfile (relative to the context)
    build:
      context: ./services/llm-service
      # The llm-service Dockerfile now uses `FROM genaimediarecommendation-base:latest`
      dockerfile: Dockerfile
    image: genaimediarecommendation-llm-service:latest
    ports:
      - "8000:8000"
    environment:
      # OLLAMA_ENDPOINT should point to the correct IP and port for your Ollama service.
      # This assumes Ollama is external. For a local container, it should be adjusted.
      - OLLAMA_ENDPOINT=http://<cloud-ip>:11434/api/generate
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:9092
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=media_recs
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - ENV=prod
    depends_on:
      kafka1:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network

  # The embedding service
  embedding-service:
    # Use the same approach as llm-service
    build:
      context: ./services/llm-service
      dockerfile: Dockerfile
    image: genaimediarecommendation-embedding-service:latest
    ports:
      - "8001:8001"
    command: uvicorn app.embedding:app --host 0.0.0.0 --port 8001
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=media_recs
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - app-network

  # The PostgreSQL database service with pgvector extension
  postgres:
    # Use the official pgvector image with a specific PostgreSQL version
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_DB: media_recs
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - ./init-pgvector.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app-network

  # The Redis caching service
  redis:
    image: redis:7
    networks:
      - app-network

  # The main Kafka broker service
  kafka1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka1
    hostname: kafka1
    ports:
      - "9092:9092"
      - "19092:19092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:19092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - ./data/kafka1:/var/lib/kafka/data
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server=kafka1:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 45s
    networks:
      - app-network

  # The Zookeeper service for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - ./data/zookeeper:/var/lib/zookeeper
    healthcheck:
      test: ["CMD-SHELL", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
